{
    "80963c21-378a-4d8d-e948-feeb9d74c049": {
        "pipeline_script": "import pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, TimestampType, DecimalType, DoubleType\nfrom datetime import datetime, timedelta\n\ndef clean_customers(df):\n\n    df_1 = df.withColumn(\n        \"email_address\",\n        F.when(F.col(\"email_address\").isNull() | (F.lower(F.col(\"email_address\")).isin(\"\", \"invalid-email\", \"none\")),\n               F.lit(\"invalid_format@example.com\"))\n        .otherwise(F.col(\"email_address\"))\n    )\n\n    df_2 = df_1.withColumn(\"first_name\", F.initcap(F.col(\"first_name\")))\n    df_3 = df_2.withColumn(\"last_name\", F.initcap(F.col(\"last_name\")))\n\n    df_4 = df_3.withColumn(\n        \"gender\",\n        F.when(F.lower(F.col(\"gender\")).isin(\"none\", \"prefer not to say\"), F.lit(\"Prefer Not To Say\"))\n        .otherwise(F.initcap(F.col(\"gender\")))\n    )\n\n    df_5 = df_4.withColumn(\"state_province\", F.upper(F.col(\"state_province\")))\n\n    df_6 = df_5.withColumn(\"phone_number\", F.regexp_replace(F.col(\"phone_number\"), \"[^0-9]\", \"\"))\n\n    df_7 = df_6.withColumn(\"registration_date\", F.to_timestamp(F.col(\"registration_date\")))\n    df_8 = df_7.filter(F.col(\"registration_date\").isNotNull())\n\n    df_9 = df_8.withColumn(\n        \"is_premium_member\",\n        F.when(F.lower(F.col(\"is_premium_member\")).isin(\"true\", \"1\", \"yes\"), F.lit(True))\n        .otherwise(F.lit(False))\n    )\n    \n    df_10 = df_9.dropDuplicates([\"customer_id\"])\n    \n    return df_10\n\ndef clean_accounts(df):\n    \n    df_0 = df.withColumn(\"balance\", F.col(\"balance\").cast(DecimalType(18, 2)))\n    df_1 = df_0.withColumn(\"balance\", F.when(F.col(\"balance\").isNull() | (F.col(\"balance\") < 0), F.lit(0.00))\n                               .otherwise(F.col(\"balance\")))\n\n    df_2 = df_1.withColumn(\n        \"account_type\",\n        F.when(F.lower(F.col(\"account_type\")).isin(\"none\", \"unspecified\"), F.lit(\"Unspecified\"))\n        .otherwise(F.initcap(F.col(\"account_type\")))\n    )\n\n    df_3 = df_2.withColumn(\"status\", F.initcap(F.col(\"status\")))\n\n    df_4 = df_3.withColumn(\"opening_date\", F.to_date(F.col(\"opening_date\")))\n    df_5 = df_4.filter(F.col(\"opening_date\").isNotNull())\n\n    df_6 = df_5.withColumn(\"interest_rate\", F.col(\"interest_rate\").cast(DoubleType()))\n    df_7 = df_6.withColumn(\"credit_limit\", F.col(\"credit_limit\").cast(DecimalType(18, 2)))\n\n    df_8 = df_7.dropDuplicates([\"account_id\"])\n    \n    return df_8\n\ndef clean_transactions(df):\n    \n    df_0 = df.withColumn(\"transaction_timestamp\", F.to_timestamp(F.col(\"transaction_timestamp\")))\n    df_1 = df_0.filter(F.col(\"transaction_timestamp\").isNotNull())\n    \n    df_2 = df_1.withColumn(\n        \"transaction_timestamp\",\n        F.when(F.col(\"transaction_timestamp\") > F.current_timestamp(), F.current_timestamp())\n        .otherwise(F.col(\"transaction_timestamp\"))\n    )\n\n    df_3 = df_2.withColumn(\"amount\", F.col(\"amount\").cast(DecimalType(18, 2)))\n    df_4 = df_3.withColumn(\"amount\", F.when(F.col(\"amount\").isNull(), F.lit(0.00)).otherwise(F.abs(F.col(\"amount\"))))\n\n    df_5 = df_4.withColumn(\n        \"transaction_type\",\n        F.when(F.lower(F.col(\"transaction_type\")).isin(\"unknown\", \"none\"), F.lit(\"Other\"))\n        .otherwise(F.initcap(F.col(\"transaction_type\")))\n    )\n\n    df_6 = df_5.withColumn(\"status\", F.initcap(F.col(\"status\")))\n\n    df_7 = df_6.dropDuplicates()\n    \n    return df_7\n\nif __name__ == \"__main__\":\n    spark.conf.set(\"spark.storage.synapse.linkedServiceName\",linked_service_name)\n    spark.conf.set(\"fs.azure.account.oauth.provider.type\",\"com.bank.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")\n\n    raw_customers_df = spark.read.load('abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet',format='parquet')\n    raw_accounts_df = spark.read.load('abfss://bank@efgh.dfs.core.windows.net/raw_accounts/accounts.parquet',format='parquet')\n    raw_transactions_df = spark.read.load('abfss://bank@efgh.dfs.core.windows.net/raw_transactions/transactions.parquet',format='parquet')\n\n    print(\"\\n--- Applying Cleaning Transformations ---\")\n    cleaned_customers_df = clean_customers(raw_customers_df)\n    cleaned_accounts_df = clean_accounts(raw_accounts_df)\n    cleaned_transactions_df = clean_transactions(raw_transactions_df)\n\n    OutputPath_customers='abfss://bank@efgh.dfs.core.windows.net/customersprod/customers.parquet'\n    OutputPath_accounts='abfss://bank@efgh.dfs.core.windows.net/accountprod/accounts.parquet'\n    OutputPath_transactions='abfss://bank@efgh.dfs.core.windows.net/transactionsprod/transactions.parquet'\n\n    cleaned_customers_df.write.mode('overwrite').parquet(OutputPath_customers)\n    cleaned_accounts_df.write.mode('overwrite').parquet(OutputPath_accounts)\n    cleaned_transactions_df.write.mode('overwrite').parquet(OutputPath_transactions)\n\n\n>>>>>\n\nSELECT\n    C.customer_id AS CustomerId,\n    C.first_name AS FirstName,\n    C.last_name AS LastName,\n    C.is_premium_member AS IsPremiumMember,\n    C.registration_date AS CustomerRegistrationDate,\n    A.account_type AS AccountType,\n    A.balance AS CurrentAccountBalance,\n    A.credit_limit AS AccountCreditLimit,\n    A.opening_date AS AccountOpeningDate,\n    SUM(T.amount) AS TotalAmountSpent,\n    COUNT(T.transaction_id) AS MonthlyTransactionCount,\n    AVG(T.amount) AS AverageMonthlyTransactionAmount\nFROM\n    Customers AS C\nINNER JOIN\n    Accounts AS A ON C.customer_id = A.customer_id\nINNER JOIN\n    Transactions AS T ON A.account_id = T.account_id\nWHERE\n    T.transaction_timestamp >= '2025-05-01' AND T.transaction_timestamp < '2025-06-01'\n    AND T.transaction_type IN ('Withdrawal', 'Purchase', 'Bill Payment', 'Transfer-Out')\n    AND T.status = 'Completed'\nGROUP BY\n    C.customer_id,\n    C.first_name,\n    C.last_name,\n    C.is_premium_member,\n    C.registration_date,\n    A.account_type,\n    A.balance,\n    A.credit_limit,\n    A.opening_date\nORDER BY\n    TotalAmountSpent DESC, C.customer_id, A.account_type;",
        "final_table": "BankTransaction",
        "metadata": "",
        "lineage": {
            "CustomerId": {
                "source_schema": "customer_id",
                "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet",
                "transformation": "C.customer_id AS CustomerId",
                "aggregation": ""
            },
            "FirstName": {
                "source_schema": "first_name",
                "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet",
                "transformation": "df_1.withColumn(\"first_name\", F.initcap(F.col(\"first_name\"))) <CODEEND> C.first_name AS FirstName",
                "aggregation": ""
            },
            "LastName": {
                "source_schema": "last_name",
                "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet",
                "transformation": "df_2.withColumn(\"last_name\", F.initcap(F.col(\"last_name\"))) <CODEEND> C.last_name AS LastName",
                "aggregation": ""
            },
            "IsPremiumMember": {
                "source_schema": "is_premium_member",
                "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet",
                "transformation": "df_8.withColumn(\"is_premium_member\", F.when(F.lower(F.col(\"is_premium_member\")).isin(\"true\", \"1\", \"yes\"), F.lit(True)) .otherwise(F.lit(False))) <CODEEND> C.is_premium_member AS IsPremiumMember",
                "aggregation": ""
            },
            "CustomerRegistrationDate": {
                "source_schema": "registration_date",
                "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet",
                "transformation": "df_6.withColumn(\"registration_date\", F.to_timestamp(F.col(\"registration_date\"))) <CODEEND> C.registration_date AS CustomerRegistrationDate",
                "aggregation": ""
            },
            "AccountType": {
                "source_schema": "account_type",
                "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_accounts/accounts.parquet",
                "transformation": "df_1.withColumn(\"account_type\", F.when(F.lower(F.col(\"account_type\")).isin(\"none\", \"unspecified\"), F.lit(\"Unspecified\")) .otherwise(F.initcap(F.col(\"account_type\")))) <CODEEND> A.account_type AS AccountType",
                "aggregation": ""
            },
            "CurrentAccountBalance": {
                "source_schema": "balance",
                "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_accounts/accounts.parquet",
                "transformation": "df.withColumn(\"balance\", F.col(\"balance\").cast(DecimalType(18, 2))) <CODEEND> df_0.withColumn(\"balance\", F.when(F.col(\"balance\").isNull() | (F.col(\"balance\") < 0), F.lit(0.00)) .otherwise(F.col(\"balance\"))) <CODEEND> A.balance AS CurrentAccountBalance",
                "aggregation": ""
            },
            "AccountCreditLimit": {
                "source_schema": "credit_limit",
                "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_accounts/accounts.parquet",
                "transformation": "df_6.withColumn(\"credit_limit\", F.col(\"credit_limit\").cast(DecimalType(18, 2))) <CODEEND> A.credit_limit AS AccountCreditLimit",
                "aggregation": ""
            },
            "AccountOpeningDate": {
                "source_schema": "opening_date",
                "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_accounts/accounts.parquet",
                "transformation": "df_3.withColumn(\"opening_date\", F.to_date(F.col(\"opening_date\"))) <CODEEND> A.opening_date AS AccountOpeningDate",
                "aggregation": ""
            },
            "TotalAmountSpent": {
                "source_schema": "amount, customer_id, first_name, last_name, is_premium_member, registration_date, account_type, balance, credit_limit, opening_date, transaction_timestamp, transaction_type, status",
                "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet; abfss://bank@efgh.dfs.core.windows.net/raw_accounts/accounts.parquet; abfss://bank@efgh.dfs.core.windows.net/raw_transactions/transactions.parquet",
                "transformation": "C.customer_id AS CustomerId <CODEEND> df_1.withColumn(\"first_name\", F.initcap(F.col(\"first_name\"))) <CODEEND> C.first_name AS FirstName <CODEEND> df_2.withColumn(\"last_name\", F.initcap(F.col(\"last_name\"))) <CODEEND> C.last_name AS LastName <CODEEND> df_8.withColumn(\"is_premium_member\", F.when(F.lower(F.col(\"is_premium_member\")).isin(\"true\", \"1\", \"yes\"), F.lit(True)) .otherwise(F.lit(False))) <CODEEND> C.is_premium_member AS IsPremiumMember <CODEEND> df_6.withColumn(\"registration_date\", F.to_timestamp(F.col(\"registration_date\"))) <CODEEND> C.registration_date AS CustomerRegistrationDate <CODEEND> df_1.withColumn(\"account_type\", F.when(F.lower(F.col(\"account_type\")).isin(\"none\", \"unspecified\"), F.lit(\"Unspecified\")) .otherwise(F.initcap(F.col(\"account_type\")))) <CODEEND> A.account_type AS AccountType <CODEEND> df.withColumn(\"balance\", F.col(\"balance\").cast(DecimalType(18, 2))) <CODEEND> df_0.withColumn(\"balance\", F.when(F.col(\"balance\").isNull() | (F.col(\"balance\") < 0), F.lit(0.00)) .otherwise(F.col(\"balance\"))) <CODEEND> A.balance AS CurrentAccountBalance <CODEEND> df_6.withColumn(\"credit_limit\", F.col(\"credit_limit\").cast(DecimalType(18, 2))) <CODEEND> A.credit_limit AS AccountCreditLimit <CODEEND> df_3.withColumn(\"opening_date\", F.to_date(F.col(\"opening_date\"))) <CODEEND> A.opening_date AS AccountOpeningDate <CODEEND> df_3.withColumn(\"amount\", F.when(F.col(\"amount\").isNull(), F.lit(0.00)).otherwise(F.abs(F.col(\"amount\")))) <CODEEND> df_2.withColumn(\"amount\", F.col(\"amount\").cast(DecimalType(18, 2))) <CODEEND> df_4.withColumn(\"transaction_type\", F.when(F.lower(F.col(\"transaction_type\")).isin(\"unknown\", \"none\"), F.lit(\"Other\")) .otherwise(F.initcap(F.col(\"transaction_type\")))) <CODEEND> df_5.withColumn(\"status\", F.initcap(F.col(\"status\"))) <CODEEND> df_0 = df.withColumn(\"transaction_timestamp\", F.to_timestamp(F.col(\"transaction_timestamp\"))) <CODEEND> df_2 = df_1.withColumn(\"transaction_timestamp\", F.when(F.col(\"transaction_timestamp\") > F.current_timestamp(), F.current_timestamp()) .otherwise(F.col(\"transaction_timestamp\"))) <CODEEND> SUM(T.amount) AS TotalAmountSpent",
                "aggregation": "SUM() GROUP BY C.customer_id, C.first_name, C.last_name, C.is_premium_member, C.registration_date, A.account_type, A.balance, A.credit_limit, A.opening_date"
            },
            "MonthlyTransactionCount": {
                "source_schema": "transaction_id, customer_id, first_name, last_name, is_premium_member, registration_date, account_type, balance, credit_limit, opening_date, transaction_timestamp, transaction_type, status",
                "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet; abfss://bank@efgh.dfs.core.windows.net/raw_accounts/accounts.parquet; abfss://bank@efgh.dfs.core.windows.net/raw_transactions/transactions.parquet",
                "transformation": "C.customer_id AS CustomerId <CODEEND> df_1.withColumn(\"first_name\", F.initcap(F.col(\"first_name\"))) <CODEEND> C.first_name AS FirstName <CODEEND> df_2.withColumn(\"last_name\", F.initcap(F.col(\"last_name\"))) <CODEEND> C.last_name AS LastName <CODEEND> df_8.withColumn(\"is_premium_member\", F.when(F.lower(F.col(\"is_premium_member\")).isin(\"true\", \"1\", \"yes\"), F.lit(True)) .otherwise(F.lit(False))) <CODEEND> C.is_premium_member AS IsPremiumMember <CODEEND> df_6.withColumn(\"registration_date\", F.to_timestamp(F.col(\"registration_date\"))) <CODEEND> C.registration_date AS CustomerRegistrationDate <CODEEND> df_1.withColumn(\"account_type\", F.when(F.lower(F.col(\"account_type\")).isin(\"none\", \"unspecified\"), F.lit(\"Unspecified\")) .otherwise(F.initcap(F.col(\"account_type\")))) <CODEEND> A.account_type AS AccountType <CODEEND> df.withColumn(\"balance\", F.col(\"balance\").cast(DecimalType(18, 2))) <CODEEND> df_0.withColumn(\"balance\", F.when(F.col(\"balance\").isNull() | (F.col(\"balance\") < 0), F.lit(0.00)) .otherwise(F.col(\"balance\"))) <CODEEND> A.balance AS CurrentAccountBalance <CODEEND> df_6.withColumn(\"credit_limit\", F.col(\"credit_limit\").cast(DecimalType(18, 2))) <CODEEND> A.credit_limit AS AccountCreditLimit <CODEEND> df_3.withColumn(\"opening_date\", F.to_date(F.col(\"opening_date\"))) <CODEEND> A.opening_date AS AccountOpeningDate <CODEEND> df_4.withColumn(\"transaction_type\", F.when(F.lower(F.col(\"transaction_type\")).isin(\"unknown\", \"none\"), F.lit(\"Other\")) .otherwise(F.initcap(F.col(\"transaction_type\")))) <CODEEND> df_5.withColumn(\"status\", F.initcap(F.col(\"status\"))) <CODEEND> df_0 = df.withColumn(\"transaction_timestamp\", F.to_timestamp(F.col(\"transaction_timestamp\"))) <CODEEND> df_2 = df_1.withColumn(\"transaction_timestamp\", F.when(F.col(\"transaction_timestamp\") > F.current_timestamp(), F.current_timestamp()) .otherwise(F.col(\"transaction_timestamp\"))) <CODEEND> COUNT(T.transaction_id) AS MonthlyTransactionCount",
                "aggregation": "COUNT() GROUP BY C.customer_id, C.first_name, C.last_name, C.is_premium_member, C.registration_date, A.account_type, A.balance, A.credit_limit, A.opening_date"
            },
            "AverageMonthlyTransactionAmount": {
                "source_schema": "amount, customer_id, first_name, last_name, is_premium_member, registration_date, account_type, balance, credit_limit, opening_date, transaction_timestamp, transaction_type, status",
                "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet; abfss://bank@efgh.dfs.core.windows.net/raw_accounts/accounts.parquet; abfss://bank@efgh.dfs.core.windows.net/raw_transactions/transactions.parquet",
                "transformation": "C.customer_id AS CustomerId <CODEEND> df_1.withColumn(\"first_name\", F.initcap(F.col(\"first_name\"))) <CODEEND> C.first_name AS FirstName <CODEEND> df_2.withColumn(\"last_name\", F.initcap(F.col(\"last_name\"))) <CODEEND> C.last_name AS LastName <CODEEND> df_8.withColumn(\"is_premium_member\", F.when(F.lower(F.col(\"is_premium_member\")).isin(\"true\", \"1\", \"yes\"), F.lit(True)) .otherwise(F.lit(False))) <CODEEND> C.is_premium_member AS IsPremiumMember <CODEEND> df_6.withColumn(\"registration_date\", F.to_timestamp(F.col(\"registration_date\"))) <CODEEND> C.registration_date AS CustomerRegistrationDate <CODEEND> df_1.withColumn(\"account_type\", F.when(F.lower(F.col(\"account_type\")).isin(\"none\", \"unspecified\"), F.lit(\"Unspecified\")) .otherwise(F.initcap(F.col(\"account_type\")))) <CODEEND> A.account_type AS AccountType <CODEEND> df.withColumn(\"balance\", F.col(\"balance\").cast(DecimalType(18, 2))) <CODEEND> df_0.withColumn(\"balance\", F.when(F.col(\"balance\").isNull() | (F.col(\"balance\") < 0), F.lit(0.00)) .otherwise(F.col(\"balance\"))) <CODEEND> A.balance AS CurrentAccountBalance <CODEEND> df_6.withColumn(\"credit_limit\", F.col(\"credit_limit\").cast(DecimalType(18, 2))) <CODEEND> A.credit_limit AS AccountCreditLimit <CODEEND> df_3.withColumn(\"opening_date\", F.to_date(F.col(\"opening_date\"))) <CODEEND> A.opening_date AS AccountOpeningDate <CODEEND> df_3.withColumn(\"amount\", F.when(F.col(\"amount\").isNull(), F.lit(0.00)).otherwise(F.abs(F.col(\"amount\")))) <CODEEND> df_2.withColumn(\"amount\", F.col(\"amount\").cast(DecimalType(18, 2))) <CODEEND> AVG(T.amount) AS AverageMonthlyTransactionAmount",
                "aggregation": "AVG() GROUP BY C.customer_id, C.first_name, C.last_name, C.is_premium_member, C.registration_date, A.account_type, A.balance, A.credit_limit, A.opening_date"
            }
        },
        "examples": {
            "example1": {
                "query": "Find the schema lineage for AverageMonthlyTransactionAmount in table dbo.BankTransaction",
                "reasoning": "We're analyzing the data lineage of the column 'AverageMonthlyTransactionAmount', tracing its path from the final output back to its original sources, transformations, and aggregations. Tracing AverageMonthlyTransactionAmount from the bottom up, we find it in the final SELECT statement, where it is defined as AVG(T.amount) AS AverageMonthlyTransactionAmount. This is an aggregation operation on the amount column from the Transactions table (aliased as T). The final SELECT statement also applies INNER JOIN operations: Customers AS C joined with Accounts AS A on C.customer_id = A.customer_id. The result of the first join joined with Transactions AS T on A.account_id = T.account_id. It includes WHERE clauses:T.transaction_timestamp >= '2025-05-01' AND T.transaction_timestamp < '2025-06-01', T.transaction_type IN ('Withdrawal', 'Purchase', 'Bill Payment', 'Transfer-Out'), and T.status = 'Completed'. These filters affect the rows included in the aggregation. The GROUP BY clause is C.customer_id, C.first_name, C.last_name, C.is_premium_member, C.registration_date: C.registration_date AS CustomerRegistrationDate, A.account_type; A.balance: A.balance AS CurrentAccountBalance; A.credit_limit: A.credit_limit AS AccountCreditLimit; A.opening_date. These schemas also have transformation from this SELECT statement. C.customer_id: C.customer_id AS CustomerI; C.first_name: C.first_name AS FirstName; C.last_name: C.last_name AS LastName; C.is_premium_member: C.is_premium_member AS IsPremiumMembe, C.registration_date, A.account_type: A.account_type AS AccountType, A.balance, A.credit_limit, A.opening_date: A.opening_date AS AccountOpeningDate. This specifies the grouping for the AVG aggregation. Therefore, we need to trace T.amount from direct transformation; C.customer_id, C.first_name, C.last_name, C.is_premium_member, C.registration_date, A.account_type, A.balance, A.credit_limit, and A.opening_date from the grouping schema; and T.transaction_timestamp, T.transaction_type, and T.status from the WHERE clause. These schemas collectively influence our target schema, AverageMonthlyTransactionAmount. Let's trace up these schema. After the SQL, we need to trace the schema back to the PySpark code. The Transactions table in the SQL query corresponds to cleaned_transactions_df in the PySpark script, which is the output of the clean_transactions function. In this transaction table, we need to these schemas, T.amount, T.transaction_timestamp, T.transaction_type, and T.status. T.amount: df_2.withColumn(\"amount\", F.col(\"amount\").cast(DecimalType(18, 2))) and df_3.withColumn(\"amount\", F.when(F.col(\"amount\").isNull(), F.lit(0.00)).otherwise(F.abs(F.col(\"amount\")))); T.transaction_timestamp: df.withColumn(\"transaction_timestamp\", F.to_timestamp(F.col(\"transaction_timestamp\"))) and df_1.withColumn(\"transaction_timestamp\", F.when(F.col(\"transaction_timestamp\") > F.current_timestamp(), F.current_timestamp()).otherwise(F.col(\"transaction_timestamp\"))). These schemas all are from the raw_transactions_df is loaded using spark: abfss://bank@efgh.dfs.core.windows.net/raw_transactions/transactions.parquet. Thus, all schemas are from transaction table that have been finished. Let's trace remaining schmeas that affect the lineage of our target schema, AverageMonthlyTransactionAmount. Next, the Accounts table in the SQL query corresponds to clean_accounts in the PySpark script, which is the output of the clean_accounts function. In this accounts table, we need to trace these schemas, A.account_type, A.balance, A.credit_limit, A.opening_date. A.account_type: df_1.withColumn(\"account_type\", F.when(F.lower(F.col(\"account_type\")).isin(\"none\", \"unspecified\"), F.lit(\"Unspecified\")) .otherwise(F.initcap(F.col(\"account_type\")))); A.balance: df.withColumn(\"balance\", F.col(\"balance\").cast(DecimalType(18, 2))) and df_0.withColumn(\"balance\", F.when(F.col(\"balance\").isNull() | (F.col(\"balance\") < 0), F.lit(0.00)) .otherwise(F.col(\"balance\"))); A.credit_limit: df_6.withColumn(\"credit_limit\", F.col(\"credit_limit\").cast(DecimalType(18, 2))); A.opening_date: df_3.withColumn(\"opening_date\", F.to_date(F.col(\"opening_date\"))). These schemas all are from the raw_accounts_df is loaded using spark: abfss://bank@efgh.dfs.core.windows.net/raw_accounts/accounts.parquet. Thus, all schemas are from accounts table that have been finished. Let's trace remaining schmeas that affect the lineage of our target schema, AverageMonthlyTransactionAmount. Finally, the Customers table in the SQL query corresponds to clean_customers in the PySpark script, which is the output of the clean_customers function. In this accounts table, we need to trace these schemas, C.customer_id, C.first_name, C.last_name, C.is_premium_member, C.registration_date. C.customer_id: there is no specific tranformation in this function; C.first_name: df_1.withColumn(\"first_name\", F.initcap(F.col(\"first_name\"))); C.last_name: df_2.withColumn(\"last_name\", F.initcap(F.col(\"last_name\"))); C.is_premium_member: df_8.withColumn(\"is_premium_member\", F.when(F.lower(F.col(\"is_premium_member\")).isin(\"true\", \"1\", \"yes\"), F.lit(True)) .otherwise(F.lit(False))); C.registration_date: df_6.withColumn(\"registration_date\", F.to_timestamp(F.col(\"registration_date\"))). These schemas all are from the raw_customers_df is loaded using spark: abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet. Thus, all schemas are from customers table that have been finished. The source schema for the raw input column ultimately includes amount. The source table for amount, transaction_timestamp, transaction_type, and status columns are from abfss://bank@efgh.dfs.core.windows.net/raw_transactions/transactions.parquet. The customer_id, first_name, last_name, is_premium_member, and registration_date columns originate from abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet. The account_type, balance, credit_limit, and opening_date columns originate from abfss://bank@efgh.dfs.core.windows.net/raw_accounts/accounts.parquet.The aggregation applied is AVG() GROUP BY C.customer_id, C.first_name, C.last_name, C.is_premium_member, C.registration_date, A.account_type, A.balance, A.credit_limit, A.opening_date.",
                "solution": {
                    "source_schema": "amount, customer_id, first_name, last_name, is_premium_member, registration_date, account_type, balance, credit_limit, opening_date, transaction_timestamp, transaction_type, status",
                    "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet; abfss://bank@efgh.dfs.core.windows.net/raw_accounts/accounts.parquet; abfss://bank@efgh.dfs.core.windows.net/raw_transactions/transactions.parquet",
                    "transformation": "C.customer_id AS CustomerId <CODEEND> df_1.withColumn(\"first_name\", F.initcap(F.col(\"first_name\"))) <CODEEND> C.first_name AS FirstName <CODEEND> df_2.withColumn(\"last_name\", F.initcap(F.col(\"last_name\"))) <CODEEND> C.last_name AS LastName <CODEEND> df_8.withColumn(\"is_premium_member\", F.when(F.lower(F.col(\"is_premium_member\")).isin(\"true\", \"1\", \"yes\"), F.lit(True)) .otherwise(F.lit(False))) <CODEEND> C.is_premium_member AS IsPremiumMember <CODEEND> df_6.withColumn(\"registration_date\", F.to_timestamp(F.col(\"registration_date\"))) <CODEEND> C.registration_date AS CustomerRegistrationDate <CODEEND> df_1.withColumn(\"account_type\", F.when(F.lower(F.col(\"account_type\")).isin(\"none\", \"unspecified\"), F.lit(\"Unspecified\")) .otherwise(F.initcap(F.col(\"account_type\")))) <CODEEND> A.account_type AS AccountType <CODEEND> df.withColumn(\"balance\", F.col(\"balance\").cast(DecimalType(18, 2))) <CODEEND> df_0.withColumn(\"balance\", F.when(F.col(\"balance\").isNull() | (F.col(\"balance\") < 0), F.lit(0.00)) .otherwise(F.col(\"balance\"))) <CODEEND> A.balance AS CurrentAccountBalance <CODEEND> df_6.withColumn(\"credit_limit\", F.col(\"credit_limit\").cast(DecimalType(18, 2))) <CODEEND> A.credit_limit AS AccountCreditLimit <CODEEND> df_3.withColumn(\"opening_date\", F.to_date(F.col(\"opening_date\"))) <CODEEND> A.opening_date AS AccountOpeningDate <CODEEND> df_4.withColumn(\"transaction_type\", F.when(F.lower(F.col(\"transaction_type\")).isin(\"unknown\", \"none\"), F.lit(\"Other\")) .otherwise(F.initcap(F.col(\"transaction_type\")))) <CODEEND> df_5.withColumn(\"status\", F.initcap(F.col(\"status\"))) <CODEEND> df_0 = df.withColumn(\"transaction_timestamp\", F.to_timestamp(F.col(\"transaction_timestamp\"))) <CODEEND> df_2 = df_1.withColumn(\"transaction_timestamp\", F.when(F.col(\"transaction_timestamp\") > F.current_timestamp(), F.current_timestamp()) .otherwise(F.col(\"transaction_timestamp\"))) <CODEEND> df_3.withColumn(\"amount\", F.when(F.col(\"amount\").isNull(), F.lit(0.00)).otherwise(F.abs(F.col(\"amount\")))) <CODEEND> df_2.withColumn(\"amount\", F.col(\"amount\").cast(DecimalType(18, 2))) <CODEEND> AVG(T.amount) AS AverageMonthlyTransactionAmount",
                    "aggregation": "AVG() GROUP BY C.customer_id, C.first_name, C.last_name, C.is_premium_member, C.registration_date, A.account_type, A.balance, A.credit_limit, A.opening_date"
                }
            },
            "example2": {
                "query": "Find the schema lineage for AccountOpeningDate in table dbo.BankTransaction",
                "reasoning": "We're analyzing the data lineage of the column 'AccountOpeningDate', tracing its path from the final output back to its original sources, transformations, and aggregations. Tracing AccountOpeningDate from the bottom up, we find it in the final SELECT statement, where it is directly selected as A.opening_date AS AccountOpeningDate. This is a direct transformation (aliasing) of the opening_date column from the Accounts table (aliased as A). The final SELECT statement also applies INNER JOIN operations: Customers AS C joined with Accounts AS A on C.customer_id = A.customer_id. The result of the first join joined with Transactions AS T on A.account_id = T.account_id. These joins combine data but do not alter the derivation of AccountOpeningDate. It includes WHERE clauses: T.transaction_timestamp >= '2025-05-01' AND T.transaction_timestamp < '2025-06-01', T.transaction_type IN ('Withdrawal', 'Purchase', 'Bill Payment', 'Transfer-Out'), and T.status = 'Completed'. These filters affect the rows selected but not the derivation of AccountOpeningDate itself.\nAfter the SQL, we need to trace the schema back to the PySpark code. The Accounts table in the SQL query corresponds to cleaned_accounts_df in the PySpark script, which is the output of the clean_accounts function. Moving to the clean_accounts function, which processes the Accounts data (aliased as A in the final SELECT statement), the opening_date column undergoes the following transformation: df_3.withColumn(\"opening_date\", F.to_date(F.col(\"opening_date\"))): The opening_date column is converted to a date type. The raw_accounts_df is loaded using spark.read.load('abfss://bank@efgh.dfs.core.windows.net/raw_accounts/accounts.parquet'). The source schema for the raw input column ultimately includes opening_date. The source table for opening_date is abfss://bank@efgh.dfs.core.windows.net/raw_accounts/accounts.parquet. No aggregation is applied in the lineage of AccountOpeningDate.",
                "solution": {
                    "source_schema": "opening_date",
                    "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_accounts/accounts.parquet",
                    "transformation": "df_3.withColumn(\"opening_date\", F.to_date(F.col(\"opening_date\"))) <CODEEND> A.opening_date AS AccountOpeningDate",
                    "aggregation": "NULL"
                }
            },
            "example3": {
                "query": "Find the schema lineage for IsPremiumMember in table dbo.BankTransaction",
                "reasoning": "We're analyzing the data lineage of the column 'IsPremiumMember', tracing its path from the final output back to its original sources, transformations, and aggregations. Tracing IsPremiumMember from the bottom up, we find it in the final SELECT statement, where it is directly selected as C.is_premium_member AS IsPremiumMember. This is a direct transformation (aliasing) of the is_premium_member column from the Customers table (aliased as C). The final SELECT statement also applies INNER JOIN operations: Customers AS C joined with Accounts AS A on C.customer_id = A.customer_id. The result of the first join joined with Transactions AS T on A.account_id = T.account_id. These joins combine data but do not alter the derivation of IsPremiumMember. It includes WHERE clauses: T.transaction_timestamp >= '2025-05-01' AND T.transaction_timestamp < '2025-06-01' T.transaction_type IN ('Withdrawal', 'Purchase', 'Bill Payment', 'Transfer-Out') T.status = 'Completed' These filters affect the rows selected but not the derivation of IsPremiumMember itself. After the SQL, we need to trace the schema back to the PySpark code. The Customers table in the SQL query corresponds to cleaned_customers_df in the PySpark script, which is the output of the clean_customers function. Moving to the clean_customers function, which processes the Customers data (aliased as C in the final SELECT statement), the is_premium_member column undergoes the following transformation: df_8.withColumn(\"is_premium_member\", F.when(F.lower(F.col(\"is_premium_member\")).isin(\"true\", \"1\", \"yes\"), F.lit(True)) .otherwise(F.lit(False))): The is_premium_member column is transformed into a boolean. If the original value (case-insensitive) is \"true\", \"1\", or \"yes\", it's set to True; otherwise, it's False. Before this transformation. The raw_customers_df is loaded using spark.read.load('abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet',format='parquet'). The source schema for the raw input column ultimately includes is_premium_member. The source table for is_premium_member is abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet. No aggregation is applied in the lineage of IsPremiumMember.",
                "solution": {
                    "source_schema": "is_premium_member",
                    "source_table": "abfss://bank@efgh.dfs.core.windows.net/raw_customers/customers.parquet",
                    "transformation": "df_8.withColumn(\"is_premium_member\", F.when(F.lower(F.col(\"is_premium_member\")).isin(\"true\", \"1\", \"yes\"), F.lit(True)) .otherwise(F.lit(False))) <CODEEND> C.is_premium_member AS IsPremiumMember",
                    "aggregation": "NULL"
                }
            }
        },
        "difficulty": 3
    },
    "13e29468-5adf-74ea-fbb0-baf181fa8a71": {
        "pipeline_script": "#DECLARE startDate DateTime = DateTime.Today.AddDays(-1);\n#DECLARE endDate DateTime = DateTime.Today;\n\n#DECLARE startTime DateTime = DateTime.Parse(@@startTime@@);\n#DECLARE endTime DateTime = DateTime.Parse(@@endTime@@);\n\n#DECLARE OrderPath string = @\"/shares/local/FoodDelivery/Orders\";\n#DECLARE OrderPattern string = @\"Order_%Y-%m-%d-%h.ss\"; \n\n#DECLARE RestaurantsPath string = string.Format(@\"/shares/local/FoodDelivery/Restaurants/restaurants{0}.ss\",@startDate);\n\n#DECLARE deliveryMetrics string = String.Format(\"/shares/local/FoodDelivery/Metrics/Hourly/AVG_Delivery_Time_%Y_%m_%d.ss?date={0}\", @endDate);\n\nraw_Orders =\n(\n    SSTREAM SPARSE STREAMSET @OrderPath\n                   PATTERN @OrderPattern\n                   RANGE __hour = [\"00\", \"23\"],\n                         __date = [\n                         @startDate,\n                         @endDate]\n);\n\nbaseOrders = \n    SELECT \n        order_id,\n        customer_id,\n        restaurant_id,\n        estimated_delivery_timestamp,\n        actual_delivery_timestamp,\n        order_timestamp,\n        order_total_amount,\n        payment_method,\n        order_status\n    FROM raw_Orders\n    WHERE order_status == \"Delivered\"\n        AND order_timestamp >= @startTime\n        AND order_timestamp < @endTime;\n\nBaseDeliveries = \n    SELECT\n        O.order_id,\n        O.restaurant_id,\n        O.customer_id,\n        O.order_timestamp,\n        O.actual_delivery_timestamp,\n        O.estimated_delivery_timestamp,\n        O.order_total_amount,\n        MinutesBetween(O.order_timestamp, O.actual_delivery_timestamp) AS DeliveryDurationMinutes,\n        IF((actual_delivery_timestamp != null) && (estimated_delivery_timestamp != null) && (MinutesBetween(order_timestamp, actual_delivery_timestamp) >= 0) && (actual_delivery_timestamp <= estimated_delivery_timestamp), 1, 0 ) AS IsOnTime,\n        IF((actual_delivery_timestamp != null) && (order_timestamp != null) && (MinutesBetween(order_timestamp, actual_delivery_timestamp) >= 0), 1, 0) AS HasValidDeliveryDuration\n    FROM\n        baseOrders AS O;\n\nRestaurantDeliveryStats = \n    SELECT\n        BD.restaurant_id,\n        COUNT(BD.order_id) AS TotalValidDeliveredOrders,\n        SUM(BD.IsOnTime) AS OnTimeOrdersCount,\n        SUM(BD.DeliveryDurationMinutes) AS TotalDeliveryDurationMinutes\n        SUM(BD.order_total_amount) AS TotalRevenueGenerated\n    FROM\n        BaseDeliveries AS BD\n    WHERE\n        BD.HasValidDeliveryDuration == 1 \n    GROUP BY BD.restaurant_id\n    HAVING COUNT(BD.order_id) >= 5;\n\nresult = \n    SELECT\n        R.restaurant_name AS RestaurantName,\n        R.cuisine_type AS CuisineType,\n        R.city AS City,\n        R.average_rating AS AverageRating,\n        RDS.TotalValidDeliveredOrders AS OrdersCount,\n        RDS.TotalDeliveryDurationMinutes.ToDouble() / RDS.TotalValidDeliveredOrders AS AverageDeliveryTimeMinutes,\n        (RDS.OnTimeOrdersCount.ToDouble() * 100.0) / RDS.TotalValidDeliveredOrders AS OnTimeDeliveryRatePercent,\n        RDS.TotalRevenueGenerated,\n        RDS.TotalRevenueGenerated.ToDouble() / RDS.TotalValidDeliveredOrders AS AverageOrderValue,\n        DaysBetween(R.onboarding_date, @endDate) AS OnboardingAgeDays\n    FROM\n        RestaurantDeliveryStats AS RDS\n    LEFT JOIN\n        RestaurantsPath AS R ON RDS.restaurant_id = R.restaurant_id;\n\nOUTPUT result TO SSTREAM @deliveryMetrics\n\n#CS\n\n    public static int MinutesBetween(DateTime start, DateTime end)\n    {\n        return (int)(end - start).TotalMinutes;\n    }\n    \n    public static int DaysBetween(DateTime start, DateTime end)\n    {\n        if (start > end)\n        {\n            return 0;\n        }\n        else\n        {\n            return (int)(end - start).TotalDays;\n        }\n    }\n\n#ENDCS",
        "final_table": "FoodDelivery",
        "metadata": "",
        "lineage": {
            "RestaurantName": {
                "source_schema": "restaurant_name",
                "source_table": "#DECLARE RestaurantsPath string = string.Format(@\"/shares/local/FoodDelivery/Restaurants/restaurants{0}.ss\",@startDate); #DECLARE startDate DateTime = DateTime.Today.AddDays(-1);",
                "transformation": "R.restaurant_name AS RestaurantName",
                "aggregation": ""
            },
            "CuisineType": {
                "source_schema": "cuisine_type",
                "source_table": "#DECLARE RestaurantsPath string = string.Format(@\"/shares/local/FoodDelivery/Restaurants/restaurants{0}.ss\",@startDate); #DECLARE startDate DateTime = DateTime.Today.AddDays(-1);",
                "transformation": "R.cuisine_type AS CuisineType",
                "aggregation": ""
            },
            "City": {
                "source_schema": "city",
                "source_table": "#DECLARE RestaurantsPath string = string.Format(@\"/shares/local/FoodDelivery/Restaurants/restaurants{0}.ss\",@startDate); #DECLARE startDate DateTime = DateTime.Today.AddDays(-1);",
                "transformation": "R.city AS City",
                "aggregation": ""
            },
            "AverageRating": {
                "source_schema": "average_rating",
                "source_table": "#DECLARE RestaurantsPath string = string.Format(@\"/shares/local/FoodDelivery/Restaurants/restaurants{0}.ss\",@startDate); #DECLARE startDate DateTime = DateTime.Today.AddDays(-1);",
                "transformation": "R.average_rating AS AverageRating",
                "aggregation": ""
            },
            "OrdersCount": {
                "source_schema": "order_id, restaurant_id",
                "source_table": "raw_Orders = (SSTREAM SPARSE STREAMSET @OrderPath PATTERN @OrderPattern RANGE __hour = [\"00\", \"23\"], __date = [ @startDate, @endDate]); #DECLARE OrderPath string = @\"/shares/local/FoodDelivery/Orders\"; #DECLARE OrderPattern string = @\"Order_%Y-%m-%d-%h.ss\"; #DECLARE startDate DateTime = DateTime.Today.AddDays(-1); #DECLARE endDate DateTime = DateTime.Today;",
                "transformation": "COUNT(BD.order_id) AS TotalValidDeliveredOrders <CODEEND> RDS.TotalValidDeliveredOrders AS OrdersCount",
                "aggregation": "COUNT() GROUP BY BD.restaurant_id"
            },
            "AverageDeliveryTimeMinutes": {
                "source_schema": "order_timestamp, actual_delivery_timestamp, order_id, restaurant_id",
                "source_table": "raw_Orders = (SSTREAM SPARSE STREAMSET @OrderPath PATTERN @OrderPattern RANGE __hour = [\"00\", \"23\"], __date = [ @startDate, @endDate]); #DECLARE OrderPath string = @\"/shares/local/FoodDelivery/Orders\"; #DECLARE OrderPattern string = @\"Order_%Y-%m-%d-%h.ss\"; #DECLARE startDate DateTime = DateTime.Today.AddDays(-2); #DECLARE endDate DateTime = DateTime.Today;",
                "transformation": "public static int MinutesBetween(DateTime start, DateTime end) { return (int)(end - start).TotalMinutes; } <CODEEND> MinutesBetween(O.order_timestamp, O.actual_delivery_timestamp) AS DeliveryDurationMinutes <CODEEND> SUM(BD.DeliveryDurationMinutes) AS TotalDeliveryDurationMinutes <CODEEND> COUNT(BD.order_id) AS TotalValidDeliveredOrders <CODEEND> RDS.TotalDeliveryDurationMinutes.ToDouble() / RDS.TotalValidDeliveredOrders AS AverageDeliveryTimeMinutes",
                "aggregation": "COUNT() GROUP BY BD.restaurant_id <CODEEND> SUM() GROUP BY BD.restaurant_id"
            },
            "OnTimeDeliveryRatePercent": {
                "source_schema": "actual_delivery_timestamp, estimated_delivery_timestamp, order_timestamp, restaurant_id, order_id",
                "source_table": "raw_Orders = (SSTREAM SPARSE STREAMSET @OrderPath PATTERN @OrderPattern RANGE __hour = [\"00\", \"23\"], __date = [ @startDate, @endDate]); #DECLARE OrderPath string = @\"/shares/local/FoodDelivery/Orders\"; #DECLARE OrderPattern string = @\"Order_%Y-%m-%d-%h.ss\"; #DECLARE startDate DateTime = DateTime.Today.AddDays(-3); #DECLARE endDate DateTime = DateTime.Today;",
                "transformation": "public static int MinutesBetween(DateTime start, DateTime end) { return (int)(end - start).TotalMinutes; } <CODEEND> IF((actual_delivery_timestamp != null) && (estimated_delivery_timestamp != null) && (MinutesBetween(order_timestamp, actual_delivery_timestamp) >= 0) && (actual_delivery_timestamp <= estimated_delivery_timestamp), 1, 0 ) AS IsOnTime <CODEEND> SUM(BD.IsOnTime) AS OnTimeOrdersCount <CODEEND> COUNT(BD.order_id) AS TotalValidDeliveredOrders <CODEEND> (RDS.OnTimeOrdersCount.ToDouble() * 100.0) / RDS.TotalValidDeliveredOrders AS OnTimeDeliveryRatePercent",
                "aggregation": "COUNT() GROUP BY BD.restaurant_id <CODEEND> SUM() GROUP BY BD.restaurant_id"
            },
            "TotalRevenueGenerated": {
                "source_schema": "order_total_amount, restaurant_id",
                "source_table": "raw_Orders = (SSTREAM SPARSE STREAMSET @OrderPath PATTERN @OrderPattern RANGE __hour = [\"00\", \"23\"], __date = [ @startDate, @endDate]); #DECLARE OrderPath string = @\"/shares/local/FoodDelivery/Orders\"; #DECLARE OrderPattern string = @\"Order_%Y-%m-%d-%h.ss\"; #DECLARE startDate DateTime = DateTime.Today.AddDays(-4); #DECLARE endDate DateTime = DateTime.Today;",
                "transformation": "SUM(BD.order_total_amount) AS TotalRevenueGenerated",
                "aggregation": "SUM() GROUP BY BD.restaurant_id"
            },
            "AverageOrderValue": {
                "source_schema": "order_total_amount, restaurant_id, order_id",
                "source_table": "raw_Orders = (SSTREAM SPARSE STREAMSET @OrderPath PATTERN @OrderPattern RANGE __hour = [\"00\", \"23\"], __date = [ @startDate, @endDate]); #DECLARE OrderPath string = @\"/shares/local/FoodDelivery/Orders\"; #DECLARE OrderPattern string = @\"Order_%Y-%m-%d-%h.ss\"; #DECLARE startDate DateTime = DateTime.Today.AddDays(-5); #DECLARE endDate DateTime = DateTime.Today;",
                "transformation": "SUM(BD.order_total_amount) AS TotalRevenueGenerated <CODEEND> COUNT(BD.order_id) AS TotalValidDeliveredOrders <CODEEND> RDS.TotalRevenueGenerated.ToDouble() / RDS.TotalValidDeliveredOrders AS AverageOrderValue",
                "aggregation": "COUNT() GROUP BY BD.restaurant_id <CODEEND> SUM() GROUP BY BD.restaurant_id"
            },
            "OnboardingAgeDays": {
                "source_schema": "onboarding_date",
                "source_table": "#DECLARE RestaurantsPath string = string.Format(@\"/shares/local/FoodDelivery/Restaurants/restaurants{0}.ss\",@startDate); #DECLARE startDate DateTime = DateTime.Today.AddDays(-1);",
                "transformation": "#DECLARE endDate DateTime = DateTime.Today; <CODEEND> public static int DaysBetween(DateTime start, DateTime end) { if (start > end) { return 0; } else { return (int)(end - start).TotalDays; } } <CODEEND> DaysBetween(R.onboarding_date, @endDate) AS OnboardingAgeDays",
                "aggregation": ""
            }
        },
        "examples": {
            "example1": {
                "query": "Find the schema lineage for OnTimeDeliveryRatePercent in table dbo.FoodDelivery",
                "reasoning": "We're analyzing the data lineage of the column 'OnTimeDeliveryRatePercent', tracing its path from the final output back to its original sources, transformations, and aggregations. Tracing OnTimeDeliveryRatePercent from the bottom up, we find it in the final SELECT statement of the result CTE, where it is defined as (RDS.OnTimeOrdersCount.ToDouble() * 100.0) / RDS.TotalValidDeliveredOrders AS OnTimeDeliveryRatePercent. This is a multi-column transformation involving OnTimeOrdersCount and TotalValidDeliveredOrders from the RestaurantDeliveryStats CTE. The result CTE also performs a LEFT JOIN with RestaurantsPath on restaurant_id and has no WHERE or ORDER BY clauses that directly impact OnTimeDeliveryRatePercent's derivation. From this CTE, we have, therefore, two schemas that need to trace up, OnTimeOrdersCount and TotalValidDeliveredOrders. Moving to RestaurantDeliveryStats, OnTimeOrdersCount is defined as SUM(BD.IsOnTime) AS OnTimeOrdersCount, which is an aggregation operation summing IsOnTime from BaseDeliveries. TotalValidDeliveredOrders is defined as COUNT(BD.order_id) AS TotalValidDeliveredOrders, which is an aggregation operation counting order_id from BaseDeliveries. Both aggregations are GROUP BY BD.restaurant_id. The RestaurantDeliveryStats CTE also applies a WHERE BD.HasValidDeliveryDuration == 1 filter and a HAVING COUNT(BD.order_id) >= 5 clause, which affect the rows considered for aggregation. From this CTE, we have, therefore, four schemas that need to trace up, IsOnTime, HasValidDeliveryDuration, restaurant_id (Group by), and order_id. Next, we trace IsOnTime, HasValidDeliveryDuration, restaurant_id, and order_id back to BaseDeliveries. IsOnTime is defined as IF((actual_delivery_timestamp != null) && (estimated_delivery_timestamp != null) && (MinutesBetween(order_timestamp, actual_delivery_timestamp) >= 0) && (actual_delivery_timestamp <= estimated_delivery_timestamp), 1, 0 ) AS IsOnTime. This is a complex transformation involving actual_delivery_timestamp, estimated_delivery_timestamp, and order_timestamp, and utilizes the user-defined function MinutesBetween. HasValidDeliveryDuration is defined as IF((actual_delivery_timestamp != null) && (order_timestamp != null) && (MinutesBetween(order_timestamp, actual_delivery_timestamp) >= 0), 1, 0) AS HasValidDeliveryDuration. This is also a complex transformation involving actual_delivery_timestamp and order_timestamp, and the MinutesBetween function. order_id and restaurant_id are directly selected from baseOrders. From this CTE, we have, thus, five schemas that need to trace up, actual_delivery_timestamp, estimated_delivery_timestamp, order_timestamp, restaurant_id, and order_id. Finally, we trace actual_delivery_timestamp, estimated_delivery_timestamp, order_timestamp, restaurant_id, and order_id back to baseOrders and then to raw_Orders. In baseOrders, actual_delivery_timestamp, estimated_delivery_timestamp, order_timestamp, restaurant_id, and order_id are directly selected from raw_Orders. The baseOrders CTE applies WHERE order_status == \"Delivered\" AND order_timestamp >= @startTime AND order_timestamp < @endTime filters, which affect the rows passed on. raw_Orders is a SSTREAM SPARSE STREAMSET reading from @OrderPath with PATTERN @OrderPattern and a RANGE on __hour and __date. The OrderPath and OrderPattern variables are declared at the top of the script, #DECLARE OrderPath string = @\"/shares/local/FoodDelivery/Orders\"; and #DECLARE OrderPattern string = @\"Order_%Y-%m-%d-%h.ss\";, along with @startDate and @endDate. startTime and endTime are also declared as DateTime.Parse(@@startTime@@) and DateTime.Parse(@@endTime@@) respectively. The source schema for the raw input columns ultimately includes actual_delivery_timestamp, estimated_delivery_timestamp, order_timestamp, order_id, and restaurant_id. The source table is raw_Orders = (SSTREAM SPARSE STREAMSET @OrderPath PATTERN @OrderPattern RANGE __hour = [\"00\", \"23\"], __date = [ @startDate, @endDate]); #DECLARE OrderPath string = @\"/shares/local/FoodDelivery/Orders\"; #DECLARE OrderPattern string = @\"Order_%Y-%m-%d-%h.ss\"; #DECLARE startTime DateTime = DateTime.Parse(@@startTime@@); #DECLARE endTime DateTime = DateTime.Parse(@@endTime@@); The aggregations applied are COUNT(BD.order_id) and SUM(BD.IsOnTime), both GROUP BY BD.restaurant_id.",
                "solution": {
                    "source_schema": "actual_delivery_timestamp, estimated_delivery_timestamp, order_timestamp, restaurant_id, order_id",
                    "source_table": "\"raw_Orders = (SSTREAM SPARSE STREAMSET @OrderPath PATTERN @OrderPattern RANGE __hour = [\"\"00\"\", \"\"23\"\"], __date = [ @startDate, @endDate]); #DECLARE OrderPath string = @\"\"/shares/local/FoodDelivery/Orders\"\"; #DECLARE OrderPattern string = @\"\"Order_%Y-%m-%d-%h.ss\"\"; #DECLARE startDate DateTime = DateTime.Today.AddDays(-1); #DECLARE endDate DateTime = DateTime.Today;\"",
                    "transformation": "public static int MinutesBetween(DateTime start, DateTime end) { return (int)(end - start).TotalMinutes; } <CODEEND> IF((actual_delivery_timestamp != null) && (estimated_delivery_timestamp != null) && (MinutesBetween(order_timestamp, actual_delivery_timestamp) >= 0) && (actual_delivery_timestamp <= estimated_delivery_timestamp), 1, 0 ) AS IsOnTime <CODEEND> SUM(BD.IsOnTime) AS OnTimeOrdersCount <CODEEND> COUNT(BD.order_id) AS TotalValidDeliveredOrders <CODEEND> (RDS.OnTimeOrdersCount.ToDouble() * 100.0) / RDS.TotalValidDeliveredOrders AS OnTimeDeliveryRatePercent",
                    "aggregation": "COUNT() GROUP BY BD.restaurant_id <CODEEND> SUM() GROUP BY BD.restaurant_id"
                }
            },
            "example2": {
                "query": "Find the schema lineage for OnboardingAgeDays in table dbo.FoodDelivery",
                "reasoning": "We're analyzing the data lineage of the column 'OnboardingAgeDays', tracing its path from the final output back to its original sources, transformations, and aggregations. Tracing OnboardingAgeDays from the bottom up, we find it in the final SELECT statement of the result CTE, where it is defined as DaysBetween(R.onboarding_date, @endDate) AS OnboardingAgeDays. This is a transformation that utilizes the user-defined function DaysBetween with R.onboarding_date from the RestaurantsPath data source and the @endDate variable. The result CTE also performs a LEFT JOIN with RestaurantDeliveryStats on restaurant_id and has no WHERE or ORDER BY clauses that directly impact OnboardingAgeDays' derivation. The DaysBetween function is defined in the C# code block: public static int DaysBetween(DateTime start, DateTime end) { if (start > end) { return 0; } else { return (int)(end - start).TotalDays; } } This function calculates the difference in days between a start and end date, returning 0 if the start date is after the end date. Thus, we need to trace up onboarding_date and endDate schemas from this CTE. Next, we trace onboarding_date back to its source. It originates from RestaurantsPath, which is declared as string.Format(@\"/shares/local/FoodDelivery/Restaurants/restaurants{0}.ss\",@startDate). This indicates that onboarding_date is read directly from a streamset file whose path is dynamically formatted using the @startDate variable. The @endDate variable is declared at the top of the script as DateTime.Today. The source schema for the raw input column ultimately includes onboarding_date. There is no need to include endDate since it is defined by variable. The source table is #DECLARE RestaurantsPath string = string.Format(@\"/shares/local/FoodDelivery/Restaurants/restaurants{0}.ss\",@startDate);. No aggregation is applied in the lineage of OnboardingAgeDays.",
                "solution": {
                    "source_schema": "onboarding_date",
                    "source_table": "#DECLARE RestaurantsPath string = string.Format(@\"/shares/local/FoodDelivery/Restaurants/restaurants{0}.ss\",@startDate); #DECLARE startDate DateTime = DateTime.Today.AddDays(-1);",
                    "transformation": "#DECLARE endDate DateTime = DateTime.Today; <CODEEND> public static int DaysBetween(DateTime start, DateTime end) { if (start > end) { return 0; } else { return (int)(end - start).TotalDays; } } <CODEEND> DaysBetween(R.onboarding_date, @endDate) AS OnboardingAgeDays",
                    "aggregation": "NULL"
                }
            }
        },
        "difficulty": 2
    },
    "2f39b775-4662-bdd4-da67-ca510943b5ec": {
        "pipeline_script": "#DECLARE movie_path string = @\"/local/user/movie.csv\";\n#DECLARE movie_studio_path string = @\"/local/user/movie_studio.csv\";\n#DECLARE studio_path string = @\"/local/user/studio.csv\";\n\nmovie = \n    SELECT \n        movie_id.ToLower() AS MovieId,\n        title,\n        release_date,\n        runtime_minutes,\n        budget,\n        language\n    FROM(\n        EXTRACT \n            movie_id : guid,\n            title : string,\n            release_date : DateTime,\n            runtime_minutes : int,\n            budget : long,\n            language : string,\n        FROM @movie_path\n        USING DefaultTextExtractor(delimiter: ',')\n    ) AS _;\n\nmovie_studio = \n    SELECT \n        movie_id.ToLower() AS MovieId,\n        studio_id.ToLower() AS StudioId,\n        production_share_percentage\n    FROM(\n        EXTRACT \n            movie_id : string,\n            studio_id : string,\n            production_share_percentage : float\n        FROM @movie_studio_path\n        USING DefaultTextExtractor(delimiter: ',')\n    ) AS _;\n\nstudio = \n    SELECT \n        studio_id.ToLower() AS StudioId,\n        studio_name,\n        headquarters_city,\n        headquarters_country,\n        website_url,\n        parent_company_id,\n    FROM(\n        EXTRACT \n            studio_id : string,\n            studio_name : string,\n            headquarters_city : string,\n            headquarters_country : string,\n            website_url : string,\n            parent_company_id : string,\n        FROM @studio_path\n        USING DefaultTextExtractor(delimiter: ',')\n    ) AS _;\n\noutput = \n    SELECT\n        M.MovieId,\n        M.title AS Title,\n        M.release_date AS ReleaseDate,\n        M.runtime_minutes AS RuntimeMinutes,\n        M.budget AS Budget,\n        M.language AS Language,\n        S.studio_name AS ProducingStudio\n    FROM\n        movie AS M\n    INNER JOIN\n        movie_studio AS MS ON M.MovieId = MS.MovieId\n    INNER JOIN\n        studio AS S ON MS.StudioId = S.StudioId\n    WHERE\n        M.budget > 100000000\n    ORDER BY\n        M.release_date DESC;\n\nOUTPUT output\nTO SSTREAM @\"/local/user/movie_metrics.ss\";",
        "final_table": "Movie",
        "metadata": "",
        "lineage": {
            "MovieId": {
                "source_schema": "movie_id",
                "source_table": "#DECLARE movie_path string = @\"/local/user/movie.csv\";",
                "transformation": "movie_id.ToLower() AS MovieId",
                "aggregation": ""
            },
            "Title": {
                "source_schema": "title",
                "source_table": "#DECLARE movie_path string = @\"/local/user/movie.csv\";",
                "transformation": "M.title AS Title",
                "aggregation": ""
            },
            "ReleaseDate": {
                "source_schema": "release_date",
                "source_table": "#DECLARE movie_path string = @\"/local/user/movie.csv\";",
                "transformation": "M.release_date AS ReleaseDate",
                "aggregation": ""
            },
            "RuntimeMinutes": {
                "source_schema": "runtime_minutes",
                "source_table": "#DECLARE movie_path string = @\"/local/user/movie.csv\";",
                "transformation": "M.runtime_minutes AS RuntimeMinutes",
                "aggregation": ""
            },
            "Budget": {
                "source_schema": "budget",
                "source_table": "#DECLARE movie_path string = @\"/local/user/movie.csv\";",
                "transformation": "M.budget AS Budget",
                "aggregation": ""
            },
            "Language": {
                "source_schema": "language",
                "source_table": "#DECLARE movie_path string = @\"/local/user/movie.csv\";",
                "transformation": "M.language AS Language",
                "aggregation": ""
            },
            "ProducingStudio": {
                "source_schema": "studio_name",
                "source_table": "#DECLARE studio_path string = @\"/local/user/studio.csv\";",
                "transformation": "S.studio_name AS ProducingStudio",
                "aggregation": ""
            }
        },
        "examples": {
            "example1": {
                "query": "Find the schema lineage for RuntimeMinutes in table dbo.Movie",
                "reasoning": "We're analyzing the data lineage of the column 'RuntimeMinutes', tracing its path from the final output back to its original sources, transformations, and aggregations. Tracing RuntimeMinutes from the bottom up, we find it in the final SELECT statement of the output CTE, where it is directly selected as M.runtime_minutes AS RuntimeMinutes. This is a direct transformation (aliasing) of the runtime_minutes column from the movie CTE. The output CTE also applies a WHERE M.budget > 100000000 filter and an ORDER BY M.release_date DESC clause, which affect the rows selected and their order but not the derivation of RuntimeMinutes itself. Thus, we can confirm there is only runtime_minutes schema associated to 'RuntimeMinutes'. Let's keep tracing up. Moving to the movie CTE, runtime_minutes is directly selected from the EXTRACT statement. This EXTRACT operation reads data from the file specified by @movie_path. Therefore, there is no other transformation or other schemas involved to generate 'RuntimeMinutes' schema. Finally, we trace the input column runtime_minutes back to its source. It is part of the schema extracted from the file located at movie_path. The source schema for the raw input column ultimately includes runtime_minutes. The source table is #DECLARE movie_path string = @\"/local/user/movie.csv\";. No aggregation is applied in the lineage of RuntimeMinutes.",
                "solution": {
                    "source_schema": "runtime_minutes",
                    "source_table": "#DECLARE movie_path string = @\"/local/user/movie.csv\";",
                    "transformation": "M.runtime_minutes AS RuntimeMinutes",
                    "aggregation": "NULL"
                }
            }
        },
        "difficulty": 1
    }
}